{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Tinkering Notebook 3a: Model-free prediction\n",
    "\n",
    "In Tinkering Notebook 2 we saw different ways to compute $v_\\pi(s)$ given that we know the dynamics $p(s', r | s, a)$. In this notebook we will see how we can learn $v_\\pi(s)$ in a model-free way using experience. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# Table of content\n",
    "* ### [1. Imports](#sec1)\n",
    "* ### [2. Monte-Carlo Methods](#sec2)\n",
    " * #### [2.1 Bias and variance](#sec2_1)\n",
    " * #### [2.2 Constant step size and non-stationary case](#sec2_2)\n",
    "* ### [3. Monte-Carlo Prediction](#sec3)\n",
    "* ### [4. Temporal Differences Prediction (TD)](#sec4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 1. Imports <a id=\"sec1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import gym_gridworld\n",
    "from IPython.display import clear_output # Used to clear the ouput of a Jupyter cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 2. Monte-Carlo Methods <a id=\"sec2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In this section we will look at the example with two dice from Lecture 4. \n",
    "\n",
    "The main point of this section is to get a better feeling for the ideas around bias and variance, and also take a look at the difference between a constant step size or a step size that decrease over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 2.1 Bias and variance <a id=\"sec2_1\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We start with by an example of throwing two dice, and we let $G$ be the sum we get from the two dices. We are interested in finding $\\mathbb{E}[ G ]$.\n",
    "With hand calculations it can be shown that $\\mathbb{E}[G]=7$. \n",
    "\n",
    "Here we instead carry out $N=1000$ throws with the two dice and compute the average value $V$. Remember that we can compute this incrementally using (see Lecture 4)\n",
    "$$\n",
    "V \\leftarrow V + \\frac{1}{n} ( G - V).\n",
    "$$\n",
    "In the code below we also store and then plot the estimated $V$ after each throw. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 1000 # Total number of throws\n",
    "V = np.zeros(N+1) # Will be used to store the mean values\n",
    "\n",
    "# V[0] is the initial value. (Should be zero to get true empircal mean)\n",
    "# V[1] is the mean after we have thrown the dices once etc.\n",
    "\n",
    "for n in range(1,N+1):\n",
    "    dice1 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "    dice2 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "    G = dice1 + dice2\n",
    "    V[n] = V[n-1] + 1/n*(G-V[n-1])\n",
    "    \n",
    "plt.plot(range(1,N+1), V[1:])\n",
    "plt.plot([1,N], [7, 7]); # True E[G]\n",
    "plt.xlabel(\"$n$ - Number of throws\")\n",
    "plt.ylabel(\"$V$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:** Re-run the code cell above a few times to see that the results are different every time. Note that the difference between each run is larger for small $n$ than large $n$.\n",
    "\n",
    "**Task:** You can try to increase $N$ in the code above, to see that $V$ really converge to 7 as $n \\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The reason that we get different results in each run is because the observations are random, so `V[n]` is random. \n",
    "\n",
    "**Bias:** Tells us how much the expected value of `V[n]` differs from the true value (7). In Lecture 4 we saw that the bias in this case is 0, so `V[n]` is an unbiased estimate for all $n$. That is: If you run the code above (infinitely) many times `V[n]` will on average be 7 for all $n$.\n",
    "\n",
    "**Variance:** Tell us how much `V[n]` will vary around the expected value if we re-run the code many times. From running the code above many times, we see that it varies more for small $n$ than for large $n$. This is consistent with the fact that the variance is $5.83/n$ (see Lecture 4), and thus decreases as $n$ increases.\n",
    "\n",
    "**Consistency:** The estimate is consistent since $V \\rightarrow \\mathbb{E}[G] = 7$ as $n\\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 2.2 Constant step size and non-stationary case <a id=\"sec2_2\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The incremental update used above can be written as \n",
    "$$\n",
    "V \\leftarrow V + \\alpha_n ( G - V)\n",
    "$$\n",
    "where $\\alpha_n = 1/n$. These types of updates will come back over and over again in the course. \n",
    "\n",
    "Sometimes we will use a constant $\\alpha \\in (0,1)$. In this section we will study constant $\\alpha$ in the simple two dice example.\n",
    "\n",
    "The effect of choosing constant $\\alpha$ is intuitively that we put less weight on observations that happened a long time ago. For example, with $\\alpha = 1$ we get $V \\leftarrow G$, i.e., we forget everything that happened before the last observation completely. (see textbook Chapter 2.5 for more discussion on this)\n",
    "\n",
    "The code below is the same as in the previous section, but with a constant step size `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1\n",
    "N = 1000 # Total number of throws\n",
    "V = np.zeros(N+1) # Will be used to store the mean values\n",
    "\n",
    "for n in range(1,N+1):\n",
    "    dice1 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "    dice2 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "    G = dice1 + dice2\n",
    "    V[n] = V[n-1] + alpha*(G-V[n-1])\n",
    "    \n",
    "plt.plot(range(1, N+1),V[1:])\n",
    "plt.plot([1,N], [7, 7]); # True E[G]\n",
    "plt.xlabel(\"$n$ - Number of throws\")\n",
    "plt.ylabel(\"$V$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:** Vary `alpha` between 0 and 1. Try at least 0.5, 0.1 and 0.01. Also compare the result with the previous section where we used $\\alpha_n = 1/n$.\n",
    "\n",
    "**Task:** Do the same as above, but with $N=10000$ to see if the estimate seems to converge as $n\\rightarrow \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "From the test above you should be able to see that:\n",
    "1. The estimate never converge to a fixed number. The reason for this is that $\\alpha$ does not go to zero, and therefor the estimate will continue to change for each new observation even as $n \\rightarrow \\infty$. Another view: The larger $\\alpha$ is, the more we focus on just the last couple of observations.\n",
    "2. But for large $n$, the estimate will vary around the true value.\n",
    "3. Smaller step size means that it takes longer to get close to the true value, but on the other hand it does not vary as much around the true value for large $n$.\n",
    "\n",
    "So, to learn fast use large $\\alpha$, but to get the result as accurate as possible as $n \\rightarrow \\infty$ use small $\\alpha$. Using the a step-size schedule such as $\\alpha_n = 1/n$, combines these insights by letting $\\alpha_n$ be large for small $n$ and small for large $n$. \n",
    "\n",
    "But why would we ever use a constant $\\alpha$ then? Some good reasons may be:\n",
    "1. Easier to implement. In more advanced settings than just computing the mean value, it is not always obvious how a good step-size schedule $\\alpha_n$ should look.\n",
    "2. It may not be important that the estimate converge to a fixed number (just that it gets close enough to the true value).\n",
    "2. If the underlying probabilities may change, it is important to continue to learn! If we let $\\alpha_n \\rightarrow 0$, then new observations will not matter very much when $n$ is large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "In the code below we first do `N_two = 1000` throws with two dice. After this one dice gets lost. So then we perform `N_one = 500` throws with only one dice (so the expected value changes from 7 to 3.5 after throw 1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_two = 1000 # Total number of throws with 2 dices\n",
    "N_one = 500 # Total number of throws with 1 dice\n",
    "N = N_one+N_two\n",
    "V = np.zeros(N+1) # Will be used to store the mean values\n",
    "\n",
    "for n in range(1,N+1):\n",
    "    if n<=N_two:\n",
    "        dice1 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "        dice2 = np.random.randint(1, 7) # Random between 1 and 6\n",
    "        G = dice1 + dice2\n",
    "    else:\n",
    "        G = np.random.randint(1,7)\n",
    "    \n",
    "    V[n] = V[n-1] + 1/n*(G-V[n-1])\n",
    "    \n",
    "plt.plot(V[1:])\n",
    "plt.plot([1, N_two, N_two, N], [7, 7, 3.5, 3.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "**Task:** Change the code above to use a constant step-size `alpha` instead of `1/n`. Try different values of `alpha`. (At least 0.01, 0.1 and 0.5)\n",
    "\n",
    "It should be clear that using a constant `alpha` (where you put less weight on old observations) gives estimates that are much faster in detecting that the expected value has changed. In a real-world RL implementation, it may be important to be able to react to a change in the underlying environment (i.e. if $p(s', r | s, a)$ changes). It could be that we control a robot, but due to wear and tear the friction between the robot and the floor changes over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 3. Monte-Carlo Prediction <a id=\"sec3\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We will now try to find an estimate of $v_\\pi(s)$ using Monte-Carlo. \n",
    "\n",
    "In Monte-Carlo prediction, we let the agent run a full episode for each state to get a trajectory\n",
    "$$\n",
    "S_0, R_1, S_1, R_2, S_2, R_3, ..., S_(T-1), R_T, S_T\n",
    "$$\n",
    "where $S_T$ is a terminal state (so no future rewards are received when this state is reached). \n",
    "\n",
    "For each state we then compute \n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\cdots + \\gamma^{T-t-1} R_T.\n",
    "$$\n",
    "Note that if we start from the end of the episode and go backwards, we can use the recursive relationship\n",
    "$$\n",
    "G_t = \\begin{cases}\n",
    "0 & \\text{if } t = T \\\\\n",
    "R_{t+1} + \\gamma G_{t+1}  &\\text{if } t<T\n",
    "\\end{cases}\n",
    "$$\n",
    "Below we implement the every-visit version of the MC-algorithm. Make sure that you understand the code. To change it to the first-visit version, you would also have to add code so that you only update $V(S_t)$ if $t$ is the first time you are in state $S_t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAgent():\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, gamma):\n",
    "        self.n_actions = n_actions\n",
    "        self.V = np.zeros(n_states)\n",
    "        self.N = np.zeros(n_states)\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Use a uniform random policy (all actions have equal probability)\n",
    "        return np.random.choice(self.n_actions)\n",
    "    \n",
    "    def learn(self, states, rewards):\n",
    "        T = len(states)\n",
    "        G = 0 # G_T = 0\n",
    "        \n",
    "        for t in reversed(range(T)): # From T-1 to 0\n",
    "            G = rewards[t+1] + self.gamma*G # G_t\n",
    "            self.N[states[t]] += 1\n",
    "            self.V[states[t]] += 1/self.N[states[t]] * (G - self.V[states[t]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To test our implementation, we use the `GymGrid-v0` environment that was also studied in Tinkering Notebook 2. With a uniform random policy and discount $\\gamma = 1$, we know that the state-value function for this environment will be "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function = np.array([ [0.,   -14., -20., -22.],\n",
    "                            [-14., -18., -20., -20.],\n",
    "                            [-20., -20., -18., -14.],\n",
    "                            [-22., -20., -14., 0.] ]).ravel() \n",
    "# ravel turn value_function into a flat array. To write it as a matrix use reshape.\n",
    "print(value_function.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "To try out the MC-algorithm, we run 1000 episodes using the uniform random policy implemented in `agent.act`. After each episode we use `agent.learn` to update our current estimate of the state-value function. We also keep track off the maximum absolute error in the estimate (which we can compute since we in this case know the true value function).\n",
    "\n",
    "We also save the history of the estimated values, so that we can see how the value in different states converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 10000\n",
    "\n",
    "env = gym.make('GridWorld-v0') # the same as in Example 3.5\n",
    "agent = MCAgent(env.observation_space.n, env.action_space.n, gamma=1)\n",
    "\n",
    "error = np.zeros(n_episodes)\n",
    "values = np.zeros((n_episodes, env.observation_space.n))\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    states = []\n",
    "    rewards = [0] # rewards[0] is not used in MC, so can set it to anything\n",
    "    done = False\n",
    "    while not done: # Run one episode with the agents policy\n",
    "        states.append(state) # Add state to list of states seen\n",
    "        action = agent.act(state) \n",
    "        state, reward, done, info = env.step(action)\n",
    "        rewards.append(reward) # Add reward to list of rewards seen\n",
    "        \n",
    "    agent.learn(states, rewards) # Update value function\n",
    "    values[i,:] = agent.V\n",
    "    # Maximum difference between true and estimated value function:\n",
    "    error[i] = np.max(np.abs(agent.V - value_function))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We first plot the maximum absolute error after each episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error)\n",
    "plt.ylabel('Error in estimates')\n",
    "plt.xlabel('Number of episodes');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "And then the estimated value of state `s` after each episode. You can change `s` to look at different states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 5 \n",
    "# Plot estimated value in state s\n",
    "plt.plot(values[:,s])\n",
    "\n",
    "# Plot true value in state s\n",
    "plt.plot([0, n_episodes], [value_function[s], value_function[s]])\n",
    "\n",
    "plt.xlabel('$n$ - number of episodes')\n",
    "plt.ylabel(\"Estimated $V(s)$ for $s={}$\".format({s}));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Tasks:\n",
    "1. Increase the number of episodes you use in training, and see if you get better estimates.\n",
    "\n",
    "2. Try to run the MC-method with a constant step-size. (You have to change the code in `MCAgent` and remember to execute the code cell again when you have made your changes.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 4. Temporal Differences Prediction (TD) <a id=\"sec4\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We now implement an agent that learn using TD. A benefit of using TD is that we do not have to wait until the end of an episode to do the updates. \n",
    "\n",
    "**Task:** Implement the TD-update in the `learn`-method of `TDAgent`. Use a constant step-size `alpha`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDAgent():\n",
    "    \n",
    "    def __init__(self, n_states, n_actions, gamma, alpha):\n",
    "        self.n_actions = n_actions\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.V = np.zeros(n_states)\n",
    "        \n",
    "    def act(self, state):\n",
    "        # Random\n",
    "        return np.random.choice(self.n_actions)\n",
    "    \n",
    "    def learn(self, state, action, reward, state_next):\n",
    "        self.V[state] = TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "We will again test our agent on the `GridWorld-v0` with environment with a uniform policy and discount $\\gamma = 1$, so the true value function is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_function = np.array([ [0.,   -14., -20., -22.],\n",
    "                            [-14., -18., -20., -20.],\n",
    "                            [-20., -20., -18., -14.],\n",
    "                            [-22., -20., -14., 0.] ]).ravel() \n",
    "# ravel turn value_function into a flat array. To write it as a matrix use reshape.\n",
    "print(value_function.reshape(4,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "The code below runs 1000 episodes. Note that we now have moved the `learn` inside the `while`-loop, since we do not have to wait for the episode to end. This also makes it possible to use the TD-method in continuing environments (that never terminates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_episodes = 1000\n",
    "s = 5 # The state we save the history for\n",
    "error = np.zeros(n_episodes)\n",
    "values = np.zeros((n_episodes, env.observation_space.n))\n",
    "\n",
    "env = gym.make('GridWorld-v0') # the same as in Example 3.5\n",
    "agent = TDAgent(env.observation_space.n, env.action_space.n, alpha=0.1, gamma=1)\n",
    "\n",
    "for i in range(n_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        state_next, reward, done, info = env.step(action)\n",
    "        agent.learn(state, action, reward, state_next)\n",
    "        state = state_next\n",
    "        \n",
    "    error[i] = np.max(np.abs(agent.V - value_function))\n",
    "    values[i, :] = agent.V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(error)\n",
    "plt.ylabel('Error in estimates')\n",
    "plt.xlabel('Number of episodes');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 5\n",
    "plt.plot(values[:, s])\n",
    "plt.plot([0, n_episodes], [value_function[s], value_function[s]])\n",
    "plt.xlabel('$n$ - number of episodes')\n",
    "plt.ylabel(\"Estimated $V(s)$ for $s={}$\".format({s}));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "### Task:\n",
    "1. Try different step sizes `alpha`, and try to use more episodes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
